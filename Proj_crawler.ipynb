{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a31c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time     #for testing \n",
    "import os       #for testing  \n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup, Comment  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "##Selenium won't be used in the final product\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0ac3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wont be using driver in the end, found a more effective method\n",
    "#driver = webdriver.Edge('.\\msedgedriver.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44d18a",
   "metadata": {},
   "source": [
    "<h4>Functions:</h4>\n",
    "    <div>*load_soup_obj - receives a url and returns a soup object.</div></n>\n",
    "    <BR>\t\n",
    "    <div>*get_page_values - receives a soup object of an anime page, gets the types (columns for the dataframe) and the values           of said types, returns them as a dictionary with keys and values.</div></n>\n",
    "    <BR>\t\n",
    "    \n",
    "   <div>*create_data_frame - receives a list of urls, each url leads to an anime page, gets each anime types and values with            get_page_values (because the run time for this function is so long O(N), I placed a percentage printer for the 10k              samples of the first data base, and a time print in the end to see how long it takes(averages in around 2hours for 10k          samples)), puts them in a DataFrame and returns it.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902a5055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_soup_obj(url):\n",
    "    path = requests.get(url)\n",
    "    soup_path = path.text\n",
    "    soup = BeautifulSoup(soup_path, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4372c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_values(soup):\n",
    "    type_list = soup.find_all(\"div\", attrs ={\"class\":\"type\"})\n",
    "    temp_value_list = soup.find_all(\"div\", attrs ={\"class\":\"value\"})\n",
    "    key_list = []\n",
    "    value_list=[]\n",
    "    for key in type_list:\n",
    "        insert_key = key.get_text(strip=True)\n",
    "        if(insert_key == 'Season'):\n",
    "            continue\n",
    "        key_list.append(insert_key)\n",
    "    for val in temp_value_list:\n",
    "        insert_value = val.get_text(strip=True)\n",
    "        value_list.append(insert_value)\n",
    "    dic = dict(zip(key_list, value_list))\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb81c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_screen(driver):\n",
    "    scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "    i = 1\n",
    "    while True:\n",
    "        # scroll one screen height each time\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "        if (screen_height) * i > scroll_height:\n",
    "            break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb517d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_frame(broken_url_list):\n",
    "    df = pd.DataFrame()\n",
    "    start_time = time.time()\n",
    "    index = 0\n",
    "    for url_list in broken_url_list:\n",
    "        for url in url_list:\n",
    "            index = index + 1\n",
    "            if(index %500 == 0):\n",
    "                print(f'%', index/100,' Complete')\n",
    "            new_url = url.get_text()\n",
    "            temp_soup = load_soup_obj(new_url)\n",
    "            url_dic = get_page_values(temp_soup)\n",
    "            df = df.append(url_dic, ignore_index=True)\n",
    "    end_time = time.time()\n",
    "    print(f\"it takes:\", end_time-start_time, \"to make a dataframe of 10k examples\")       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f981c6",
   "metadata": {},
   "source": [
    "<h4>Main body:</h4>\n",
    "    <div>get the url list from the sitemap .xml page, and creates a dataframe using the create_data_frame function, then saves it to     a .csv format in order to avoid running the crawler everytime we need the data. \n",
    "    (formerly I used selenium to scroll on the page to get each url, but the load time, scroll time and getting the url were       really inefficient, thus I searched for a more efficient method and found the sitemap)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5325af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_url_list = []\n",
    "main_soup = load_soup_obj('https://anilist.co/sitemap/anime-1.xml')\n",
    "broken_url_list.append(main_soup.find_all(\"loc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ed3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_data_frame(broken_url_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2157c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Anime_Data_Frame_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
